{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python368jvsc74a57bd060aa7568c9db8113868ebef0220b161e96389b06f6ba9eb98d46b6b0f2cf6a72",
   "display_name": "Python 3.6.8 64-bit ('benchmark': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import swifter\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import sys\n",
    "import spacy\n",
    "import re\n",
    "import time\n",
    "import scispacy\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from note_processing.heuristic_tokenize import sent_tokenize_rules \n",
    "from mimic3models.preprocessing import Discretizer_Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOS_PATH = \"/mnt/data01/mimic-3/benchmark-small/length-of-stay\"\n",
    "LISTFILES = [\"test_listfile.csv\", \"train_listfile.csv\", \"val_listfile.csv\"]\n",
    "#LISTFILES = [\"train_listfile.csv\"]\n",
    "NOTEABR = \"bert\"    # bert or BioSent2Vec\n",
    "MAX_SENT = 80       # Max sentence len for window\n",
    "EMBEDDIM = 768      # Embedding dimesions\n",
    "TIMESTEP = 1        # Timestep for discretizer (not for window)\n",
    "WINDOW = 5          # Window size\n",
    "STEPSIZE = 1        # Step size for window\n",
    "WORKERS = 5\n",
    "\n",
    "discretizer = Discretizer_Notes(timestep=TIMESTEP,\n",
    "                          store_masks=False,  # Not currently supported\n",
    "                          impute_strategy='zero',\n",
    "                          start_time='zero',\n",
    "                          sent_dim=80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episode_embeddings(tup):\n",
    "        ts_filename = tup[\"stay\"]\n",
    "        test_train = tup[\"set\"]\n",
    "\n",
    "        ret = []\n",
    "        patient_id = re.findall(r'[0-9]+_', ts_filename)[0][:-1]\n",
    "        episode = re.findall(r'episode[0-9]+_', ts_filename)[-1][7:-1]\n",
    "\n",
    "        par_dir = os.path.abspath(os.path.join(LOS_PATH, os.pardir))\n",
    "\n",
    "        filename = f\"episode{episode}_notes_{NOTEABR}.parquet\"\n",
    "        filename = os.path.join(par_dir, test_train, patient_id, filename)\n",
    "        \n",
    "        columns = [\"Hours\", \"CATEGORY\", \"DESCRIPTION\", \"TEXT_EMBEDDING\"]\n",
    "        try:\n",
    "            df = pd.read_parquet(filename)\n",
    "            columns = list(df.columns)\n",
    "            df[\"Hours\"] = df.index\n",
    "            columns.insert(0, \"Hours\")\n",
    "            ret = df[columns]\n",
    "        except BaseException as e:\n",
    "            print(f\"Fail for patient: {patient_id} with error: {str(e)}\")\n",
    "            # TODO Remove hack\n",
    "            ret = None\n",
    "\n",
    "        return ret, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_window(tensors):\n",
    "    data = np.zeros((MAX_SENT, EMBEDDIM))\n",
    "    tensors = np.flip(tensors, 0)\n",
    "\n",
    "    mask = [0] * WINDOW\n",
    "    for i, row in enumerate(tensors):\n",
    "        embedding = row\n",
    "\n",
    "        if row[0,0] == 0:\n",
    "            continue\n",
    "\n",
    "        start = 0\n",
    "        while start < MAX_SENT and data[start, 0] != 0:\n",
    "            start += 1\n",
    "\n",
    "        if start >= MAX_SENT:\n",
    "            # DROPPED_SENTS += embedding.shape[0]\n",
    "            continue\n",
    "\n",
    "        over = (start + embedding.shape[0]) - MAX_SENT\n",
    "        # DROPPED_SENTS += over if over > 0 else 0\n",
    "        remaining = embedding.shape[0] if over < 0 else MAX_SENT - start\n",
    "        remain_embedding = np.stack(embedding[:remaining])\n",
    "        data[start:start+remaining] = remain_embedding\n",
    "        mask[i] = 1\n",
    "\n",
    "    return data, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(tensor):\n",
    "    windows = np.zeros((int((tensor.shape[0]-(WINDOW-1))/STEPSIZE), MAX_SENT, EMBEDDIM))\n",
    "    # print(f\"Tensor shape is {tensor.shape}\")\n",
    "    # print(f\"Windows shape is {windows.shape}\")\n",
    "    masks = []\n",
    "\n",
    "    for i in range(0, windows.shape[0], STEPSIZE):\n",
    "        window, mask = pack_window(tensor[i:i+WINDOW])\n",
    "        windows[i] = window\n",
    "        masks.append(mask)\n",
    "\n",
    "    masks = np.stack(masks)\n",
    "\n",
    "    return windows, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_episode(tup):\n",
    "        ts_filename = tup[\"stay\"]\n",
    "        test_train = tup[\"set\"]\n",
    "\n",
    "        patient_id = re.findall(r'[0-9]+_', ts_filename)[0][:-1]\n",
    "        episode = re.findall(r'episode[0-9]+_', ts_filename)[-1][7:-1]\n",
    "        \n",
    "        df, filename = get_episode_embeddings(tup)\n",
    "        \n",
    "        try:\n",
    "            if df is not None:\n",
    "                df_np = df.to_numpy() # (#notes, #sent(var), #embedding)\n",
    "\n",
    "                # Create tensor with impution\n",
    "                (tensor, header) = discretizer.transform(df_np, header=None, end=int(tup[\"period_length\"]))\n",
    "                tensor, masks = create_windows(tensor)\n",
    "\n",
    "                outfile = f\"episode{episode}_notes_{NOTEABR}_window{WINDOW}-{STEPSIZE}_tensor.parquet\"\n",
    "                out_df = pd.DataFrame([{\"TEXT_WINDOW_EMBEDDING\": tensor.tolist()}])\n",
    "                out_df.to_parquet(os.path.join(os.path.dirname(filename), outfile))\n",
    "            else:\n",
    "                outfile = None\n",
    "        except BaseException as e:\n",
    "            print(f\"Failed to process {patient_id}:{episode} because: {str(e)}\")\n",
    "            outfile = None\n",
    "\n",
    "        return outfile\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e91a3028d3564968b4cde45ebd87c600"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/39 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "209d4b7564f64c64a6cb18110102ba98"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fail for patient: 107 with error: /mnt/data01/mimic-3/benchmark-small/train/107/episode2_notes_bert.parquet\n",
      "Fail for patient: 165 with error: /mnt/data01/mimic-3/benchmark-small/train/165/episode1_notes_bert.parquet\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "195dbc61a9564c7dbda00fe751f2e4ee"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "for listfile in LISTFILES:\n",
    "    filename = os.path.join(LOS_PATH, listfile)\n",
    "    df = pd.read_csv(filename)\n",
    "    df[\"set\"] = re.findall(r'(?:test|train|val)', listfile)[0]\n",
    "    df[\"set\"] = df[\"set\"].apply(lambda x: \"train\" if x == \"val\" else x)\n",
    "\n",
    "    group_df = df.groupby([\"stay\",\"set\"], as_index=False)[\"period_length\"].agg(\"max\")\n",
    "\n",
    "    # For each group build the imputed note tensor\n",
    "    tensor_df = group_df.copy().reset_index()\n",
    "    tensor_df[\"tensor\"] = tensor_df.progress_apply(process_episode, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "def get_window_indices(data_len, window_len = 4):\n",
    "    i = 0\n",
    "    indices = []\n",
    "    while i <= data_len-window_len:\n",
    "        indices.append([i+j for j in range(window_len)])\n",
    "        i +=1\n",
    "    return indices\n",
    "\n",
    "len(get_window_indices(37,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                               TEXT_WINDOW_EMBEDDING\n",
       "0  [[[0.4503781795501709, -0.14556898176670074, 0..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TEXT_WINDOW_EMBEDDING</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[[[0.4503781795501709, -0.14556898176670074, 0...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df = pd.read_parquet(\"/mnt/data01/mimic-3/benchmark-small/test/345/episode1_notes_bert_window5-1_tensor.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(87, 80, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "np.stack([np.stack(x) for x in df[\"TEXT_WINDOW_EMBEDDING\"].iloc[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "embedding = np.stack([np.stack(x) for x in df[\"TEXT_WINDOW_EMBEDDING\"].iloc[0]])\n",
    "print(embedding[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}