{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python368jvsc74a57bd060aa7568c9db8113868ebef0220b161e96389b06f6ba9eb98d46b6b0f2cf6a72",
   "display_name": "Python 3.6.8 64-bit ('benchmark': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import sys\n",
    "import spacy\n",
    "import re\n",
    "import time\n",
    "import scispacy\n",
    "import glob\n",
    "import os\n",
    "from pandarallel import pandarallel\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from note_processing.heuristic_tokenize import sent_tokenize_rules \n",
    "from mimic3models.preprocessing import Discretizer_Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO: Pandarallel will run on 2 workers.\nINFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "LOS_PATH = \"/mnt/data01/mimic-3/benchmark-notes/length-of-stay\"\n",
    "LISTFILES = [\"test_listfile.csv\", \"train_listfile.csv\", \"val_listfile.csv\"]\n",
    "# LISTFILES = [\"val_listfile.csv\"]\n",
    "NOTEABR = \"bert\"\n",
    "EMBEDDIM = 768\n",
    "WORKERS = 2\n",
    "TIMESTEP = 5\n",
    "\n",
    "discretizer = Discretizer_Notes(timestep=TIMESTEP,\n",
    "                          store_masks=False,  # Not currently supported\n",
    "                          impute_strategy='previous',\n",
    "                          start_time='zero',\n",
    "                          sent_dim=80)\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=WORKERS)                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episode_embeddings(tup):\n",
    "        ts_filename = tup[\"stay\"]\n",
    "        test_train = tup[\"set\"]\n",
    "\n",
    "        ret = []\n",
    "        patient_id = re.findall(r'[0-9]+_', ts_filename)[0][:-1]\n",
    "        episode = re.findall(r'episode[0-9]+_', ts_filename)[-1][7:-1]\n",
    "\n",
    "        par_dir = os.path.abspath(os.path.join(LOS_PATH, os.pardir))\n",
    "\n",
    "        filename = f\"episode{episode}_notes_{NOTEABR}.parquet\"\n",
    "        filename = os.path.join(par_dir, test_train, patient_id, filename)\n",
    "        \n",
    "        columns = [\"Hours\", \"CATEGORY\", \"DESCRIPTION\", \"TEXT_EMBEDDING\"]\n",
    "        try:\n",
    "            df = pd.read_parquet(filename)\n",
    "            columns = list(df.columns)\n",
    "            df[\"Hours\"] = df.index\n",
    "            columns.insert(0, \"Hours\")\n",
    "            ret = df[columns]\n",
    "        except BaseException as e:\n",
    "            print(f\"Fail for patient: {patient_id} with error: {str(e)}\")\n",
    "            # TODO Remove hack\n",
    "            ret = None\n",
    "\n",
    "        return ret, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_episode(tup):\n",
    "        ts_filename = tup[\"stay\"]\n",
    "        test_train = tup[\"set\"]\n",
    "\n",
    "        patient_id = re.findall(r'[0-9]+_', ts_filename)[0][:-1]\n",
    "        episode = re.findall(r'episode[0-9]+_', ts_filename)[-1][7:-1]\n",
    "        \n",
    "        df, filename = get_episode_embeddings(tup)\n",
    "        \n",
    "        if df is not None:\n",
    "            df_np = df.to_numpy()\n",
    "\n",
    "            # Create tensor with impution\n",
    "            (tensor, header) = discretizer.transform(df_np, header=None, end=int(tup[\"period_length\"]))\n",
    "\n",
    "            outfile = f\"episode{episode}_notes_{NOTEABR}_bin{TIMESTEP}_tensor.parquet\"\n",
    "            out_df = pd.DataFrame([{\"TEXT_BIN_EMBEDDING\": tensor.tolist()}])\n",
    "            out_df.to_parquet(os.path.join(os.path.dirname(filename), outfile))\n",
    "        else:\n",
    "            outfile = None\n",
    "\n",
    "        return outfile\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  6.68it/s]\n",
      " 13%|█▎        | 5/39 [00:00<00:00, 34.23it/s]Fail for patient: 107 with error: /mnt/data01/mimic-3/benchmark-small/train/107/episode2_notes_bert.parquet\n",
      " 77%|███████▋  | 30/39 [00:02<00:01,  8.40it/s]Fail for patient: 165 with error: /mnt/data01/mimic-3/benchmark-small/train/165/episode1_notes_bert.parquet\n",
      "100%|██████████| 39/39 [00:03<00:00, 10.81it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.45it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for listfile in LISTFILES:\n",
    "    filename = os.path.join(LOS_PATH, listfile)\n",
    "    df = pd.read_csv(filename)\n",
    "    df[\"set\"] = re.findall(r'(?:test|train|val)', listfile)[0]\n",
    "    df[\"set\"] = df[\"set\"].apply(lambda x: \"train\" if x == \"val\" else x)\n",
    "\n",
    "    group_df = df.groupby([\"stay\",\"set\"], as_index=False)[\"period_length\"].agg(\"max\")\n",
    "\n",
    "    # For each group build the imputed note tensor\n",
    "    tensor_df = group_df.copy().reset_index()\n",
    "    tensor_df[\"tensor\"] = tensor_df.progress_apply(process_episode, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                            stay   set  period_length\n",
      "0  10000_episode1_timeseries.csv  test           31.0\n",
      "1  10011_episode1_timeseries.csv  test          332.0\n",
      "2  10012_episode1_timeseries.csv  test           33.0\n",
      "3  10019_episode1_timeseries.csv  test           31.0\n",
      "4   1001_episode1_timeseries.csv  test           21.0\n",
      "       period_length\n",
      "count    6265.000000\n",
      "mean       87.970790\n",
      "std       127.748688\n",
      "min         5.000000\n",
      "25%        28.000000\n",
      "50%        49.000000\n",
      "75%        91.000000\n",
      "max      1992.000000\n",
      "                            stay    set  period_length\n",
      "0  10003_episode1_timeseries.csv  train           35.0\n",
      "1  10004_episode1_timeseries.csv  train          251.0\n",
      "2  10004_episode2_timeseries.csv  train           17.0\n",
      "3  10006_episode1_timeseries.csv  train           39.0\n",
      "4  10007_episode1_timeseries.csv  train          198.0\n",
      "       period_length\n",
      "count   29168.000000\n",
      "mean       86.050295\n",
      "std       123.965907\n",
      "min         5.000000\n",
      "25%        27.000000\n",
      "50%        48.000000\n",
      "75%        89.000000\n",
      "max      2803.000000\n",
      "                            stay    set  period_length\n",
      "0  10022_episode1_timeseries.csv  train           44.0\n",
      "1  10033_episode1_timeseries.csv  train           25.0\n",
      "2  10037_episode1_timeseries.csv  train           24.0\n",
      "3  10046_episode1_timeseries.csv  train           69.0\n",
      "4  10071_episode1_timeseries.csv  train           25.0\n",
      "       period_length\n",
      "count    6355.000000\n",
      "mean       88.001259\n",
      "std       125.641505\n",
      "min         5.000000\n",
      "25%        28.000000\n",
      "50%        48.000000\n",
      "75%        90.000000\n",
      "max      1842.000000\n"
     ]
    }
   ],
   "source": [
    "for listfile in LISTFILES:\n",
    "    filename = os.path.join(LOS_PATH, listfile)\n",
    "    df = pd.read_csv(filename)\n",
    "    df[\"set\"] = re.findall(r'(?:test|train|val)', listfile)[0]\n",
    "    df[\"set\"] = df[\"set\"].apply(lambda x: \"train\" if x == \"val\" else x)\n",
    "\n",
    "    group_df = df.groupby([\"stay\",\"set\"], as_index=False)[\"period_length\"].agg(\"max\")\n",
    "\n",
    "    print(group_df.head())\n",
    "    print(group_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}