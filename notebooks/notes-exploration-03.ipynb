{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python368jvsc74a57bd060aa7568c9db8113868ebef0220b161e96389b06f6ba9eb98d46b6b0f2cf6a72",
   "display_name": "Python 3.6.8 64-bit ('benchmark': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import spacy\n",
    "import re\n",
    "import time\n",
    "import scispacy\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from note_processing.heuristic_tokenize import sent_tokenize_rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_DIR = '/mnt/data01/mimic-3/benchmark-small/test/345' #this path will contain tokenized notes. This dir will be the input dir for create_pretrain_data.sh\n",
    "\n",
    "#this is the path to mimic data if you're reading from a csv. Else uncomment the code to read from database below\n",
    "MIMIC_NOTES_PATHS = ['/mnt/data01/mimic-3/benchmark-small/test',\n",
    "                     '/mnt/data01/mimic-3/benchmark-small/train']  \n",
    "\n",
    "WORKERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Load note files: 100%|██████████| 2/2 [00:00<00:00, 164.53it/s]\n",
      "Total note files: 43\n",
      "Total unprocessed files: 2\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       hours category description text filename\n",
       "count      0        0           0    0        0\n",
       "unique     0        0           0    0        0\n",
       "top      NaN      NaN         NaN  NaN      NaN\n",
       "freq     NaN      NaN         NaN  NaN      NaN"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hours</th>\n      <th>category</th>\n      <th>description</th>\n      <th>text</th>\n      <th>filename</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "all_files = []\n",
    "\n",
    "for path in MIMIC_NOTES_PATHS:\n",
    "    files = glob.glob(path + \"/*/*_notes.csv\")\n",
    "    all_files += files\n",
    "\n",
    "print(\"\\nTotal note files: \" + str(len(all_files)))\n",
    "all_files = [f for f in all_files if not os.path.exists(f[:-4] + '_sent.csv')]\n",
    "print(\"Total unprocessed files: \" + str(len(all_files)))\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in tqdm(all_files, desc=\"Load note files\"):\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df[\"filename\"] = filename\n",
    "    li.append(df)\n",
    "\n",
    "notes = pd.concat(li, axis=0, ignore_index=True)\n",
    "notes.columns= notes.columns.str.lower()\n",
    "notes.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [hours, category, description, text, filename]\n",
       "Index: []"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hours</th>\n      <th>category</th>\n      <th>description</th>\n      <th>text</th>\n      <th>filename</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "notes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               hours description  text filename\n",
       "               count       count count    count\n",
       "category                                       \n",
       "General           23          23    23       23\n",
       "Nursing          502         502   502      502\n",
       "Nursing/other     50          50    50       50\n",
       "Nutrition         13          13    13       13\n",
       "Pharmacy           2           2     2        2\n",
       "Physician        333         333   333      333\n",
       "Radiology        119         119   119      119\n",
       "Rehab Services     8           8     8        8\n",
       "Respiratory       75          75    75       75\n",
       "Social Work        1           1     1        1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>hours</th>\n      <th>description</th>\n      <th>text</th>\n      <th>filename</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>count</th>\n      <th>count</th>\n      <th>count</th>\n      <th>count</th>\n    </tr>\n    <tr>\n      <th>category</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>General</th>\n      <td>23</td>\n      <td>23</td>\n      <td>23</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>Nursing</th>\n      <td>502</td>\n      <td>502</td>\n      <td>502</td>\n      <td>502</td>\n    </tr>\n    <tr>\n      <th>Nursing/other</th>\n      <td>50</td>\n      <td>50</td>\n      <td>50</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>Nutrition</th>\n      <td>13</td>\n      <td>13</td>\n      <td>13</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>Pharmacy</th>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>Physician</th>\n      <td>333</td>\n      <td>333</td>\n      <td>333</td>\n      <td>333</td>\n    </tr>\n    <tr>\n      <th>Radiology</th>\n      <td>119</td>\n      <td>119</td>\n      <td>119</td>\n      <td>119</td>\n    </tr>\n    <tr>\n      <th>Rehab Services</th>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>Respiratory</th>\n      <td>75</td>\n      <td>75</td>\n      <td>75</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>Social Work</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "notes.groupby(\"category\").agg(['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aflanders:\n",
    "# This code will split the notes into natural sentence boundaries separated by \\n\n",
    "# which can then be fed into sentence embedding models such as BIO-ClinicalBert or \n",
    "# BioSentVec\n",
    "#\n",
    "# This frame and the next are largly from format_mimic_for_BERT.py in EmilyAlsentzer/clinicalBERT\n",
    "# I have updated the code to work with spacy 3.0 and made some other changes\n",
    "#\n",
    "# Example:\n",
    "# THis is a \n",
    "# single \n",
    "# sentence. and another sentence.\n",
    "\n",
    "# THis is a single sentence.\\n\n",
    "# and another sentence.\\n\n",
    "\n",
    "from spacy.language import Language\n",
    "\n",
    "#setting sentence boundaries\n",
    "@Language.component('sbd_component')\n",
    "def sbd_component(doc):\n",
    "    for i, token in enumerate(doc[:-2]):\n",
    "        # define sentence start if period + titlecase token\n",
    "        if token.text == '.' and doc[i+1].is_title:\n",
    "            doc[i+1].sent_start = True\n",
    "        if token.text == '-' and doc[i+1].text != '-':\n",
    "            doc[i+1].sent_start = True\n",
    "    return doc\n",
    "\n",
    "#convert de-identification text into one token\n",
    "# aflanders: no need to pass in the next separate, is available in processed_text\n",
    "# def fix_deid_tokens(text, processed_text):\n",
    "def fix_deid_tokens(doc):\n",
    "    deid_regex  = r\"\\[\\*\\*.{0,15}.*?\\*\\*\\]\" \n",
    "\n",
    "    indexes = [m.span() for m in re.finditer(deid_regex, doc.text, flags=re.IGNORECASE)]\n",
    "\n",
    "    for start,end in indexes:\n",
    "        # processed_text.merge(start_idx=start,end_idx=end)\n",
    "        # aflanders: Make compatible with latest version fo spacy\n",
    "        try:\n",
    "            span = doc.char_span(start, end)\n",
    "            if span is not None:\n",
    "                with doc.retokenize() as retokenizer:\n",
    "                    # retokenizer.merge(processed_text[start:end+1])\n",
    "                    retokenizer.merge(span)\n",
    "        except:\n",
    "            print(f'Error with: {text}')\n",
    "                \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_section(section, note, processed_sections):\n",
    "    # perform spacy processing on section\n",
    "    processed_section = nlp(section['sections'])\n",
    "    # processed_section = fix_deid_tokens(section['sections'], processed_section)\n",
    "    processed_section = fix_deid_tokens(processed_section)\n",
    "    processed_sections.append(processed_section)\n",
    "\n",
    "def process_note_helper(note):\n",
    "    # split note into sections\n",
    "    note_sections = sent_tokenize_rules(note)\n",
    "    processed_sections = []\n",
    "    section_frame = pd.DataFrame({'sections':note_sections})\n",
    "    section_frame.apply(process_section, args=(note,processed_sections,), axis=1)\n",
    "    return(processed_sections)\n",
    "\n",
    "def process_text(sent, note):\n",
    "    sent_text = sent['sents'].text\n",
    "    if len(sent_text) > 0 and sent_text.strip() != '\\n':\n",
    "        if '\\n' in sent_text:\n",
    "            sent_text = sent_text.replace('\\n', ' ')\n",
    "        note['text'] += sent_text + '\\n'  \n",
    "\n",
    "def get_sentences(processed_section, note):\n",
    "    # get sentences from spacy processing\n",
    "    sent_frame = pd.DataFrame({'sents': list(processed_section['sections'].sents)})\n",
    "    sent_frame.apply(process_text, args=(note,), axis=1)\n",
    "\n",
    "def process_note(note):\n",
    "    try:\n",
    "        note_text = note['text'] #unicode(note['text'])\n",
    "        note['text'] = ''\n",
    "        processed_sections = process_note_helper(note_text)\n",
    "        ps = {'sections': processed_sections}\n",
    "        ps = pd.DataFrame(ps)\n",
    "        ps.apply(get_sentences, args=(note,), axis=1)\n",
    "        return note \n",
    "    except Exception as e:\n",
    "        # pass\n",
    "        print ('error processing note', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Begin reading notes\nNumber of notes: 50\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function __main__.sbd_component>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# %time\n",
    "\n",
    "#category = [\"Nursing\", \"Nursing/other\", 'General', 'Physician ']  # or None\n",
    "category = [\"Nursing/other\"]  # or None\n",
    "\n",
    "# start = time.time()\n",
    "# tqdm.pandas()\n",
    "\n",
    "print('Begin reading notes')\n",
    "\n",
    "if category != None:\n",
    "    notes = notes[notes['category'].isin(category)]\n",
    "print('Number of notes: %d' %len(notes.index))\n",
    "# notes['ind'] = list(range(len(notes.index)))\n",
    "\n",
    "nlp = spacy.load('en_core_sci_md', disable=['tagger','ner', 'lemmatizer'])\n",
    "nlp.add_pipe('sbd_component', before='parser')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "filenames = list(notes[\"filename\"].unique().tolist())\n",
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\nINFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=4), Label(value='0 / 4'))), HBox(c…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b52b21f3166c4e29964c16c9831f4236"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "formatted_notes = notes.parallel_apply(process_note, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Bio_ClinicalBERT model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't bother trying to run the pipeline without a GPU\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "pipe = pipeline('feature-extraction', model=model, \n",
    "                tokenizer=tokenizer, device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.8.1+cu102\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7f424ed61546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getCompiledVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cuda compiled version'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/bin/anaconda3/envs/benchmark/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;34mr\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/bin/anaconda3/envs/benchmark/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.__version__)\n",
    "print(torch.cuda.current_device())\n",
    "print(torch._C._cuda_getCompiledVersion(), 'cuda compiled version')\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.03993183,  0.28045335, -0.22612181, ..., -0.36056122,\n",
       "         0.02098022, -0.07117227],\n",
       "       [ 0.26059052,  0.28010404, -0.19884744, ..., -0.41535103,\n",
       "         0.55648178, -0.36985204]])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "features = pipe(['Respiratory CAre Pt received from ED intubated for airway protection.And then another sentenc',\n",
    "                  'Coughing and gagging with Sx, swallowing frequently with irritation of ETT.']  ,\n",
    "                pad_to_max_length=True)\n",
    "features = np.squeeze(features)\n",
    "features = features[:,0,:]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_embeddings(text, pipe):\n",
    "    sents = text.split('\\n')[:-1]\n",
    "    #sents = list(map(lambda x: x[:50], sents))\n",
    "    start_idx = 0\n",
    "    while True:\n",
    "        try:\n",
    "            sent_features = pipe(sents[start_idx:] ,pad_to_max_length=True)\n",
    "        except BaseException as e:\n",
    "            start_idx += 1\n",
    "\n",
    "            if start_idx >= len(sents):\n",
    "                print(\"\\nError in get_embeddings()\")\n",
    "                print('# of sentences: '+ str(len(sents)))\n",
    "                sent_len = [len(x) for x in sents]\n",
    "                print(sent_len)\n",
    "                sent_features = None\n",
    "                break\n",
    "\n",
    "            print(\"Dropping sentence: \" + sents[start_idx-1])\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    if sent_features is not None:\n",
    "        try:\n",
    "            sent_features = np.squeeze(sent_features)[:,0,:]\n",
    "        except:\n",
    "            sent_features = None\n",
    "    \n",
    "    return sent_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# strings = formatted_notes[\"text\"].tolist()\n",
    "# doc = '\\n'.join(strings)\n",
    "# sents = doc.split('\\n')[:-1]\n",
    "# features = pipe(sents ,\n",
    "#                 pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 50/50 [00:43<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "formatted_notes[\"embeddings\"] = formatted_notes[\"text\"].progress_apply(get_embeddings, args=(pipe,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(7, 768)\n(47, 768)\n(5, 768)\n(71, 768)\n(43, 768)\n(5, 768)\n(51, 768)\n(45, 768)\n(6, 768)\n(28, 768)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(formatted_notes[\"embeddings\"].iloc[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             hours       category description  \\\n",
       "count    50.000000             50          50   \n",
       "unique         NaN              1           1   \n",
       "top            NaN  Nursing/other      Report   \n",
       "freq           NaN             50          50   \n",
       "mean     45.648222            NaN         NaN   \n",
       "std      31.821967            NaN         NaN   \n",
       "min       4.174444            NaN         NaN   \n",
       "25%      21.070764            NaN         NaN   \n",
       "50%      44.101111            NaN         NaN   \n",
       "75%      62.639236            NaN         NaN   \n",
       "max     129.083611            NaN         NaN   \n",
       "\n",
       "                                                     text  \\\n",
       "count                                                  50   \n",
       "unique                                                 50   \n",
       "top     Nursing Admission/Progress Note (1335-\\n1900) ...   \n",
       "freq                                                    1   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "                                                 filename       ind  \\\n",
       "count                                                  50  50.00000   \n",
       "unique                                                  7       NaN   \n",
       "top     /mnt/data01/mimic-3/benchmark-small/train/191/...       NaN   \n",
       "freq                                                   20       NaN   \n",
       "mean                                                  NaN  24.50000   \n",
       "std                                                   NaN  14.57738   \n",
       "min                                                   NaN   0.00000   \n",
       "25%                                                   NaN  12.25000   \n",
       "50%                                                   NaN  24.50000   \n",
       "75%                                                   NaN  36.75000   \n",
       "max                                                   NaN  49.00000   \n",
       "\n",
       "                                               embeddings  \n",
       "count                                                  50  \n",
       "unique                                                 50  \n",
       "top     [[0.38307225704193115, 0.34203147888183594, 0....  \n",
       "freq                                                    1  \n",
       "mean                                                  NaN  \n",
       "std                                                   NaN  \n",
       "min                                                   NaN  \n",
       "25%                                                   NaN  \n",
       "50%                                                   NaN  \n",
       "75%                                                   NaN  \n",
       "max                                                   NaN  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hours</th>\n      <th>category</th>\n      <th>description</th>\n      <th>text</th>\n      <th>filename</th>\n      <th>ind</th>\n      <th>embeddings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>50.000000</td>\n      <td>50</td>\n      <td>50</td>\n      <td>50</td>\n      <td>50</td>\n      <td>50.00000</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>50</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>NaN</td>\n      <td>Nursing/other</td>\n      <td>Report</td>\n      <td>Nursing Admission/Progress Note (1335-\\n1900) ...</td>\n      <td>/mnt/data01/mimic-3/benchmark-small/train/191/...</td>\n      <td>NaN</td>\n      <td>[[0.38307225704193115, 0.34203147888183594, 0....</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>NaN</td>\n      <td>50</td>\n      <td>50</td>\n      <td>1</td>\n      <td>20</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>45.648222</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>24.50000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>31.821967</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>14.57738</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>4.174444</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.00000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>21.070764</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12.25000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>44.101111</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>24.50000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>62.639236</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>36.75000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>129.083611</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>49.00000</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "formatted_notes.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      hours       category description  \\\n",
       "3  7.351111  Nursing/other      Report   \n",
       "\n",
       "                                                text  \\\n",
       "3  Respiratory CAre Pt received from ED intubated...   \n",
       "\n",
       "                                            filename  ind  \\\n",
       "3  /mnt/data01/mimic-3/benchmark-small/test/345/e...    0   \n",
       "\n",
       "                                          embeddings  \n",
       "3  [[-0.23393520712852478, -0.19446520507335663, ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hours</th>\n      <th>category</th>\n      <th>description</th>\n      <th>text</th>\n      <th>filename</th>\n      <th>ind</th>\n      <th>embeddings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>7.351111</td>\n      <td>Nursing/other</td>\n      <td>Report</td>\n      <td>Respiratory CAre Pt received from ED intubated...</td>\n      <td>/mnt/data01/mimic-3/benchmark-small/test/345/e...</td>\n      <td>0</td>\n      <td>[[-0.23393520712852478, -0.19446520507335663, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "formatted_notes.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "type(formatted_notes[\"embeddings\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out a new notes file with the embeddings\n",
    "# aflanders: This is going to take too long and take up too much space\n",
    "# The embeddings will be longer than the notes themselves. Each patient/episode\n",
    "# can go from 500Kb to 18Mb\n",
    "\n",
    "# filenames = [filename[\"filename\"].value]\n",
    "# formatted_notes.columns= formatted_notes.columns.str.capitalize()\n",
    "# formatted_notes.rename(columns={\"Embeddings\":\"Bert embeddings\"}, inplace=True)\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# filenames = list(formatted_notes[\"Filename\"].unique().tolist())\n",
    "# for filename in tqdm(filenames, desc=\"Writing embedding files\"):\n",
    "#     df = formatted_notes[formatted_notes[\"Filename\"] == filename][[\"Hours\", \"Category\", \"Description\", \"Bert embeddings\"]]\n",
    "#     write_file = filename.replace(\"_notes.csv\", \"_notes_bert.csv\")\n",
    "#     with open(write_file, \"w\") as f:\n",
    "#         df.to_csv(f, index_label='Hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['/mnt/data01/mimic-3/benchmark-small/test/345/episode1_notes.csv', '/mnt/data01/mimic-3/benchmark-small/train/124/episode2_notes.csv', '/mnt/data01/mimic-3/benchmark-small/train/191/episode2_notes.csv', '/mnt/data01/mimic-3/benchmark-small/train/222/episode3_notes.csv', '/mnt/data01/mimic-3/benchmark-small/train/222/episode4_notes.csv', '/mnt/data01/mimic-3/benchmark-small/train/109/episode8_notes.csv', '/mnt/data01/mimic-3/benchmark-small/train/109/episode7_notes.csv']\n"
     ]
    }
   ],
   "source": [
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}